{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d6eb2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Launching parallel scraping...\n",
      "\n",
      "üìÑ Full Log Summary:\n",
      "========================================\n",
      "üöò [EXEED-LX.1] Started scraping for Make: exeed, Model: lx\n",
      "‚û°Ô∏è [EXEED-LX.2] [INFO] Loaded 4 cars from exeedlx: https://dubai.dubizzle.com/motors/rental-cars/exeed/lx\n",
      "‚û°Ô∏è [EXEED-LX.3]    üî¢ Filtered exeed, lx cars: 4\n",
      "‚û°Ô∏è [EXEED-LX.4]       ‚Ü™Ô∏è Sub-page: https://dubai.dubizzle.com/motors/rental-cars/exeed/lx/2024/11/13/brand-new-2025-free-delivery-exeed-lx-2-752---578acc4219944546b1aa17ae005b216e/\n",
      "‚û°Ô∏è [EXEED-LX.5]       ‚Ü™Ô∏è Sub-page: https://dubai.dubizzle.com/motors/rental-cars/exeed/lx/2025/6/19/exeed-lx-comfort-no-deposit-option-availab-2-591---9eda52f10fe44e8d96fc13cbfb3f3636/\n",
      "‚û°Ô∏è [EXEED-LX.6]       ‚Ü™Ô∏è Sub-page: https://dubai.dubizzle.com/motors/rental-cars/exeed/lx/2025/1/26/affordable-suv-starting-from-99-aed-2-306---ca185182e6584eba80a348b1444d7c9d/\n",
      "‚û°Ô∏è [EXEED-LX.7]       ‚Ü™Ô∏è Sub-page: https://dubai.dubizzle.com/motors/rental-cars/exeed/lx/2024/12/13/no-deposit-no-hidden-charges-free-delivery-2-973---74d28758af6c42c78aac894c2c52c604/\n",
      "‚û°Ô∏è [EXEED-LX.8] üõë Browser closed.\n",
      "\n",
      "üìÅ Saved: d:\\Project-Analytics\\Dubai_Projects\\DXB_Rates\\output\\dubizzle_rentals_30062025.xlsx\n",
      "\n",
      "‚úÖ All scraping complete.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "import traceback\n",
    "from bs4 import BeautifulSoup\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "# ================= LOGGING ====================\n",
    "class ThreadLogger:\n",
    "    def __init__(self):\n",
    "        self.logs = []\n",
    "        self.context = None\n",
    "        self.lock = threading.Lock()\n",
    "        self.counter = 0\n",
    "\n",
    "    def set_context(self, context):\n",
    "        self.context = context\n",
    "        self.counter = 0\n",
    "\n",
    "    def log(self, message, indent=0, emoji=\"‚û°Ô∏è\"):\n",
    "        self.counter += 1\n",
    "        prefix = f\"{'    ' * indent}{emoji} [{self.context}.{self.counter}]\"\n",
    "        with self.lock:\n",
    "            self.logs.append(f\"{prefix} {message}\")\n",
    "\n",
    "    def get_logs(self):\n",
    "        return self.logs.copy()\n",
    "\n",
    "# ========== CONFIG FILE ==========\n",
    "config_path = Path.cwd() / \"config/make_model.csv\"\n",
    "config = pd.read_csv(config_path, usecols=[\"make\", \"year\", \"dubizzle_model\"], low_memory=False)\n",
    "\n",
    "# Filter out rows where dubizzle_model is blank or NaN\n",
    "config = config[config[\"dubizzle_model\"].notna() & (config[\"dubizzle_model\"].str.strip() != \"\")]\n",
    "\n",
    "today_str = datetime.today().strftime(\"%d%m%Y\")\n",
    "filename = Path.cwd() / f\"output/dubizzle_rentals_{today_str}.xlsx\"\n",
    "\n",
    "# ========== BROWSER FACTORY ==========\n",
    "\n",
    "def make_fast_firefox(headless=True):\n",
    "    options = Options()\n",
    "    options.headless = headless\n",
    "\n",
    "    # Load pages as quickly as possible\n",
    "    options.page_load_strategy = \"eager\"\n",
    "\n",
    "    # Private browsing mode (no cookies/cache)\n",
    "    options.add_argument(\"-private\")\n",
    "\n",
    "    # Reduce unnecessary rendering and animation\n",
    "    options.set_preference(\"permissions.default.image\", 2)  # block all images\n",
    "    options.set_preference(\"media.autoplay.default\", 0)  # allow autoplay (can reduce blocking)\n",
    "    options.set_preference(\"browser.shell.checkDefaultBrowser\", False)\n",
    "    options.set_preference(\"browser.startup.page\", 0)  # skip startup tabs\n",
    "    options.set_preference(\"browser.startup.homepage\", \"about:blank\")\n",
    "    options.set_preference(\"startup.homepage_welcome_url\", \"about:blank\")\n",
    "    options.set_preference(\"startup.homepage_welcome_url.additional\", \"about:blank\")\n",
    "\n",
    "    # Disable smooth effects\n",
    "    options.set_preference(\"toolkit.cosmeticAnimations.enabled\", False)\n",
    "    options.set_preference(\"layout.css.animation.enabled\", False)\n",
    "    options.set_preference(\"layout.css.transition.enabled\", False)\n",
    "    options.set_preference(\"general.smoothScroll\", False)\n",
    "    options.set_preference(\"ui.prefersReducedMotion\", 1)\n",
    "\n",
    "    # Disable prefetching and caching\n",
    "    options.set_preference(\"network.dns.disablePrefetch\", True)\n",
    "    options.set_preference(\"network.prefetch-next\", False)\n",
    "    options.set_preference(\"network.http.use-cache\", False)\n",
    "\n",
    "    # Reduce tab/thread overhead\n",
    "    options.set_preference(\"dom.ipc.processCount\", 1)\n",
    "    options.set_preference(\"browser.tabs.remote.autostart\", False)\n",
    "\n",
    "    # Optional: disable fonts (can break rendering, use with caution)\n",
    "    options.set_preference(\"gfx.downloadable_fonts.enabled\", False)\n",
    "\n",
    "    # Set paths\n",
    "    options.binary_location = r\"C:\\Program Files\\Mozilla Firefox\\firefox.exe\"\n",
    "    service = Service(executable_path=r\"C:\\drivers\\geckodriver.exe\")\n",
    "\n",
    "    return webdriver.Firefox(service=service, options=options)\n",
    "\n",
    "# ========== DATA CLEANING HELPERS ==========\n",
    "\n",
    "def extract_numeric(text):\n",
    "    if not text:\n",
    "        return None\n",
    "    nums = ''.join(filter(str.isdigit, text))\n",
    "    return int(nums) if nums else None\n",
    "\n",
    "def scroll_to_bottom(driver, pause=2, max_attempts=3):\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    for attempt in range(max_attempts):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(pause)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "# Define custom sort orders\n",
    "contract_order = CategoricalDtype([\"daily\", \"weekly\", \"monthly\"], ordered=True)\n",
    "\n",
    "# ========== MAIN PAGE SCRAPER ==========\n",
    "\n",
    "def scrape_dubizzle_car_data(driver, url):\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Set implicit wait for soft fallback\n",
    "    driver.implicitly_wait(5)\n",
    "\n",
    "    # Wait for car listings to appear\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"#listing-card-wrapper a[data-testid^='listing-']\"))\n",
    "        )\n",
    "    except:\n",
    "        print(f\"‚ö†Ô∏è Timeout waiting for listings on {url}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Scroll to bottom to load all listings\n",
    "    scroll_to_bottom(driver)\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Parse final DOM\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    car_cards = soup.select(\"#listing-card-wrapper a[data-testid^='listing-']\")\n",
    "\n",
    "    extracted = []\n",
    "\n",
    "    for card in car_cards:\n",
    "        try:\n",
    "            full_url = \"https://dubai.dubizzle.com\" + card.get(\"href\", \"\")\n",
    "\n",
    "            name_tags = card.select(\"h3[data-testid^='heading-text']\")\n",
    "            car_name = name_tags[0].text.strip() if len(name_tags) > 0 else \"\"\n",
    "            model = name_tags[1].text.strip() if len(name_tags) > 1 else \"\"\n",
    "            variant = name_tags[2].text.strip() if len(name_tags) > 2 else \"\"\n",
    "\n",
    "            year_tag = card.select_one(\"h3[data-testid='listing-year']\")\n",
    "            year = extract_numeric(year_tag.text) if year_tag else None\n",
    "\n",
    "            is_featured = \"Yes\" if card.select_one(\"[data-testid='featured-badge']\") else \"\"\n",
    "\n",
    "            extracted.append({\n",
    "                \"sub-url\": full_url,\n",
    "                \"make\": car_name,\n",
    "                \"model\": model,\n",
    "                \"variant\": variant,\n",
    "                \"year\": year,\n",
    "                \"is_featured\": is_featured\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"‚ùå Error parsing card:\", e)\n",
    "            continue\n",
    "\n",
    "    return pd.DataFrame(extracted)\n",
    "\n",
    "\n",
    "# ========== DETAIL PAGE SCRAPER ==========\n",
    "\n",
    "def scrape_dubizzle_detail(driver, url):\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Set implicit wait for soft fallback\n",
    "    driver.implicitly_wait(5)\n",
    "\n",
    "    # Wait for description or contract section to load\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"h6[data-testid='listing-sub-heading'], h5[data-testid^='rental-price-']\"))\n",
    "        )\n",
    "    except:\n",
    "        print(f\"‚ö†Ô∏è Timeout waiting for detail elements {url}.\")\n",
    "        return []\n",
    "\n",
    "\n",
    "    # Scroll to bottom to load all listings\n",
    "    scroll_to_bottom(driver)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    enriched_data = []\n",
    "\n",
    "    def safe_select(selector, attr=\"text\", many=False):\n",
    "        try:\n",
    "            elements = soup.select(selector)\n",
    "            if not elements:\n",
    "                return \"\" if not many else []\n",
    "            if attr == \"text\":\n",
    "                return [el.get_text(strip=True) for el in elements] if many else elements[0].get_text(strip=True)\n",
    "            else:\n",
    "                return [el.get(attr, \"\") for el in elements] if many else elements[0].get(attr, \"\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è safe_select failed for {selector}: {e}\")\n",
    "            return \"\" if not many else []\n",
    "\n",
    "    # Dealer link\n",
    "    dealer_url = safe_select(\"a[data-testid='view-all-cars']\", attr=\"href\")\n",
    "    if dealer_url and dealer_url.startswith(\"/\"):\n",
    "        dealer_url = \"https://dubai.dubizzle.com\" + dealer_url\n",
    "\n",
    "    contract_list = []\n",
    "    for contract in [\"daily\", \"weekly\", \"monthly\"]:\n",
    "        price = safe_select(f\"h5[data-testid='rental-price-{contract}']\")\n",
    "        if not price:\n",
    "            continue\n",
    "\n",
    "        unlimited_text = safe_select(f\"p[data-testid='unlimited-kms-{contract}']\").lower()\n",
    "        unlimited = unlimited_text == \"unlimited kilometers\"\n",
    "\n",
    "        raw_km = safe_select(f\"p[data-testid='allowed-kms-{contract}']\")\n",
    "        km_match = re.search(r\"\\d+\\s*km\", raw_km, re.IGNORECASE) if raw_km else None\n",
    "        km_limit = km_match.group() if km_match else None\n",
    "\n",
    "        extra_km = safe_select(f\"p[data-testid='additional-kms-{contract}']\")\n",
    "\n",
    "        contract_list.append({\n",
    "            \"contract\": contract,\n",
    "            \"base_price\": extract_numeric(price),\n",
    "            \"mileage\": \"Unlimited\" if unlimited else km_limit,\n",
    "            \"mileage_note\": \"\" if unlimited else extra_km\n",
    "        })\n",
    "\n",
    "    # Common info\n",
    "    description = safe_select(\"h6[data-testid='listing-sub-heading']\")\n",
    "    sub_description = safe_select(\"p[data-testid='description']\")\n",
    "    posted_on = safe_select(\"p[data-testid='posted-on']\")\n",
    "    dealer_name = safe_select(\"p[data-testid='name']\")\n",
    "    dealer_type = safe_select(\"p[data-testid='type']\")\n",
    "    min_driver_age = safe_select(\"[data-ui-id='details-value-minimum_driver_age']\")\n",
    "    deposit = extract_numeric(safe_select(\"[data-ui-id='details-value-security_deposit']\"))\n",
    "    refund_period = safe_select(\"[data-ui-id='details-value-security_refund_period']\")\n",
    "    location = safe_select(\"div[data-testid='listing-location-map']\")\n",
    "\n",
    "    # Final result\n",
    "    for contract_entry in contract_list:\n",
    "        enriched_data.append({\n",
    "            \"sub-url\": url,\n",
    "            \"description\": description,\n",
    "            \"sub_description\": sub_description,\n",
    "            \"posted_on\": posted_on,\n",
    "            \"dealer_name\": dealer_name,\n",
    "            \"dealer_type\": dealer_type,\n",
    "            \"dealer_page\": dealer_url,\n",
    "            **contract_entry,\n",
    "            \"minimum_driver_age\": min_driver_age,\n",
    "            \"deposit\": deposit,\n",
    "            \"refund_period\": refund_period,\n",
    "            \"location\": location\n",
    "        })\n",
    "\n",
    "    return enriched_data\n",
    "\n",
    "# ========== PARALLEL SCRAPE WRAPPER ==========\n",
    "\n",
    "def scrape_main_for_make_model(make, model, logger):\n",
    "    context = f\"{make.upper()}-{model.upper()}\"\n",
    "    logger.set_context(context)\n",
    "    logger.log(f\"Started scraping for Make: {make}, Model: {model}\", 0, \"üöò\")\n",
    "\n",
    "    url = f\"https://dubai.dubizzle.com/motors/rental-cars/{make}/{model}\"\n",
    "    \n",
    "    driver = make_fast_firefox(headless=True)\n",
    "    local_main_dataframes, local_detail_dicts = [], []\n",
    "\n",
    "    try:\n",
    "        df = scrape_dubizzle_car_data(driver, url)\n",
    "        logger.log(f\"[INFO] Loaded {len(df)} cars from {make}{model}: {url}\")\n",
    "\n",
    "        # Drop the unwanted urls based on make, model and year as defined in Config\n",
    "        merge_cols = [\"make\", \"model\", \"year\"]\n",
    "        df_norm = df.copy()\n",
    "        \n",
    "        missing_cols = [col for col in merge_cols if col not in df_norm.columns]\n",
    "        if missing_cols:\n",
    "            logger.log(f\"‚ùå Skipping {make}-{model} ‚Äî missing columns: {missing_cols}\")\n",
    "            return [], [], logger.get_logs()\n",
    "        \n",
    "        # Normalize for join\n",
    "        for col in merge_cols:\n",
    "            df_norm[col] = df_norm[col].astype(str).str.upper()\n",
    "        config_norm = config.rename(columns={\"dubizzle_model\": \"model\"}).copy()\n",
    "        for col in merge_cols:\n",
    "            config_norm[col] = config_norm[col].astype(str).str.upper()\n",
    "        \n",
    "        filtered_df = df_norm.merge(config_norm, on=merge_cols, how=\"inner\")\n",
    "        filtered_df[\"year\"] = pd.to_numeric(filtered_df[\"year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "        \n",
    "        if filtered_df.empty:\n",
    "            logger.log(f\"‚ö†Ô∏è No {make} {model} results found, inline with Config.\")\n",
    "            return [], [], logger.get_logs()\n",
    "\n",
    "        local_main_dataframes.append(filtered_df)\n",
    "        logger.log(f\"   üî¢ Filtered {make}, {model} cars: {len(filtered_df)}\")\n",
    "\n",
    "        # Detail scraping\n",
    "        for _, car_row in filtered_df.iterrows():\n",
    "            sub_url = car_row[\"sub-url\"]\n",
    "            logger.log(f\"      ‚Ü™Ô∏è Sub-page: {sub_url}\")\n",
    "            try:\n",
    "                detail_df = scrape_dubizzle_detail(driver, sub_url)\n",
    "                local_detail_dicts.extend(detail_df)\n",
    "            except Exception as e:\n",
    "                logger.log(f\"‚ùå Error scraping sub-page: {e}\")\n",
    "                continue\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.log(f\"‚ùå Error scraping main page {make}-{model}: {e}\")\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        logger.log(\"üõë Browser closed.\")\n",
    "\n",
    "    return local_main_dataframes, local_detail_dicts, logger.get_logs()\n",
    "\n",
    "\n",
    "# ========== MAIN RUN WITH THREADING ==========\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Launching parallel scraping...\")\n",
    "\n",
    "    main_dataframes, detail_dicts, all_logs = [], [], []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        futures = []\n",
    "        unique_config = config.drop_duplicates(subset=['make', 'dubizzle_model'])\n",
    "        for _, row in unique_config.iterrows():\n",
    "            make = row['make']\n",
    "            model = row['dubizzle_model']\n",
    "            logger = ThreadLogger()\n",
    "            futures.append(executor.submit(scrape_main_for_make_model, make, model, logger))\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                local_main_df, local_detail_df, logs = future.result()\n",
    "                main_dataframes.extend(local_main_df)\n",
    "                detail_dicts.extend(local_detail_df)\n",
    "                all_logs.extend(logs)  # ‚¨ÖÔ∏è all logs from this thread\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Thread failed for make={make}, model={model}: {e}\")\n",
    "                traceback.print_exc()  # ‚úÖ See full error\n",
    "\n",
    "    # ========== POST PROCESSING ==========\n",
    "    print(\"\\nüìÑ Full Log Summary:\\n\" + \"=\"*40)\n",
    "    for line in all_logs:\n",
    "        print(line)\n",
    "\n",
    "    main_df = pd.concat(main_dataframes, ignore_index=True)\n",
    "    detail_df = pd.DataFrame(detail_dicts)\n",
    "    \n",
    "    # Title\n",
    "    mg_models = {'mg3': '3', 'mg5': '5'}\n",
    "\n",
    "    # Cleaned model column\n",
    "    model_cleaned = main_df['model'].str.lower().replace(mg_models)\n",
    "\n",
    "    # Construct title\n",
    "    main_df['title'] = (\n",
    "        main_df['make'].str.lower() + ' ' +\n",
    "        model_cleaned + ' ' +\n",
    "        main_df['year'].astype(str)\n",
    "    )\n",
    "\n",
    "    # Check if the 'base_price' column exists in the DataFrame\n",
    "    if 'base_price' in detail_df.columns:\n",
    "        detail_df['savings'] = 0\n",
    "        detail_df['offered_price'] = detail_df['base_price']\n",
    "    else:\n",
    "        print(\"‚ùå 'base_price' column not found in final_df\")\n",
    "\n",
    "    # Merge\n",
    "    final_df = pd.merge(main_df, detail_df, on=\"sub-url\", how=\"left\")\n",
    "    \n",
    "    # Apply the categorical types\n",
    "    final_df[\"contract\"] = final_df[\"contract\"].astype(contract_order)\n",
    "\n",
    "    # Sort the DataFrame correctly\n",
    "    df_sorted = final_df.sort_values(by=[\"contract\", \"sub-url\"]).reset_index(drop=True)\n",
    "    \n",
    "    # Reorder columns    \n",
    "    dubizzle_df = df_sorted[[\"sub-url\", \"title\", \"make\", \"model\", \"year\", \"is_featured\", \"variant\",\n",
    "        \"contract\", \"base_price\", \"savings\", \"offered_price\", \"description\", \"sub_description\",\n",
    "        \"posted_on\", \"dealer_name\", \"dealer_type\", \"dealer_page\", \"mileage\", \"mileage_note\",\n",
    "        \"minimum_driver_age\", \"deposit\", \"refund_period\", \"location\"]]\n",
    "    \n",
    "    # Export as Excel    \n",
    "    dubizzle_df.to_excel(filename, index=False)\n",
    "    print(f\"\\nüìÅ Saved: {filename}\")\n",
    "    print(\"\\n‚úÖ All scraping complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
