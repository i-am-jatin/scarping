{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d075a8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Launching parallel scraping...\n",
      "\n",
      "üìÑ Full Log Summary:\n",
      "========================================\n",
      "üöò [NISSAN-SUNNY.1] Started scraping for Make: nissan, Model: sunny\n",
      "‚û°Ô∏è [NISSAN-SUNNY.2] [INFO] Loaded 12 cars from Daily: https://drive.yango.com/search/all/nissan/sunny?since=1751349600000&until=1751436000000\n",
      "‚û°Ô∏è [NISSAN-SUNNY.3]    üî¢ Filtered nissan, sunny cars: 10\n",
      "‚û°Ô∏è [NISSAN-SUNNY.4]      ‚Ü™Ô∏è Opening sub-page: https://drive.yango.com/book?object_id=01d75403-9337-4af1-874d-68167bdeeb8b&since=1751349600000&until=1751436000000\n",
      "‚û°Ô∏è [NISSAN-SUNNY.5]      ‚Ü™Ô∏è Opening sub-page: https://drive.yango.com/book?object_id=00e6de4f-d679-4ad1-9a6c-e4ed93089f83&since=1751349600000&until=1751436000000\n",
      "‚û°Ô∏è [NISSAN-SUNNY.6]      ‚Ü™Ô∏è Opening sub-page: https://drive.yango.com/book?object_id=06c6e027-8222-4033-9c57-7698c9d43fe1&since=1751349600000&until=1751436000000\n",
      "‚û°Ô∏è [NISSAN-SUNNY.7]      ‚Ü™Ô∏è Opening sub-page: https://drive.yango.com/book?object_id=08fa858d-c3de-4ed7-afda-c42bb0c30221&since=1751349600000&until=1751436000000\n",
      "‚û°Ô∏è [NISSAN-SUNNY.8]      ‚Ü™Ô∏è Opening sub-page: https://drive.yango.com/book?object_id=3decd157-0c4a-4775-9c71-bbd54a894b7a&since=1751349600000&until=1751436000000\n",
      "‚û°Ô∏è [NISSAN-SUNNY.9]      ‚Ü™Ô∏è Opening sub-page: https://drive.yango.com/book?object_id=0421a69c-346a-4cb4-b117-0b151976c324&since=1751349600000&until=1751436000000\n",
      "‚û°Ô∏è [NISSAN-SUNNY.10]      ‚Ü™Ô∏è Opening sub-page: https://drive.yango.com/book?object_id=00ec41d3-7ba9-4d62-b433-4376b55b9f88&since=1751349600000&until=1751436000000\n",
      "‚û°Ô∏è [NISSAN-SUNNY.11]      ‚Ü™Ô∏è Opening sub-page: https://drive.yango.com/book?object_id=0c468854-4e01-477c-806f-0fd18c99bb41&since=1751349600000&until=1751436000000\n",
      "‚û°Ô∏è [NISSAN-SUNNY.12]      ‚Ü™Ô∏è Opening sub-page: https://drive.yango.com/book?object_id=01a74e7a-c831-4874-9719-2fae6961d04d&since=1751349600000&until=1751436000000\n",
      "‚û°Ô∏è [NISSAN-SUNNY.13]      ‚Ü™Ô∏è Opening sub-page: https://drive.yango.com/book?object_id=0eb3e66f-affe-4e86-9b96-5950f1532d3f&since=1751349600000&until=1751436000000\n",
      "‚û°Ô∏è [NISSAN-SUNNY.14] [INFO] Loaded 12 cars from Weekly: https://drive.yango.com/search/all/nissan/sunny?since=1751349600000&until=1751954400000\n",
      "‚û°Ô∏è [NISSAN-SUNNY.15]    üî¢ Filtered nissan, sunny cars: 8\n",
      "‚û°Ô∏è [NISSAN-SUNNY.16]      ‚Ü™Ô∏è Opening sub-page: https://drive.yango.com/book?object_id=059bf7b9-5c85-492e-a08f-a339f0507097&since=1751349600000&until=1751954400000\n",
      "‚û°Ô∏è [NISSAN-SUNNY.17]      ‚Ü™Ô∏è Opening sub-page: https://drive.yango.com/book?object_id=069a6aac-9c04-41d8-98ba-6502dd99d22c&since=1751349600000&until=1751954400000\n",
      "‚û°Ô∏è [NISSAN-SUNNY.18]      ‚Ü™Ô∏è Opening sub-page: https://drive.yango.com/book?object_id=06576274-72e6-46e8-b829-643561360680&since=1751349600000&until=1751954400000\n",
      "‚û°Ô∏è [NISSAN-SUNNY.19]      ‚Ü™Ô∏è Opening sub-page: https://drive.yango.com/book?object_id=01bed153-d941-4ed8-a069-6cdb077071b7&since=1751349600000&until=1751954400000\n",
      "‚û°Ô∏è [NISSAN-SUNNY.20]      ‚Ü™Ô∏è Opening sub-page: https://drive.yango.com/book?object_id=120f02dc-837d-40c8-a311-145d246c4927&since=1751349600000&until=1751954400000\n",
      "‚û°Ô∏è [NISSAN-SUNNY.21]      ‚Ü™Ô∏è Opening sub-page: https://drive.yango.com/book?object_id=07f2e26f-dfab-4f96-8a62-d73e2d302722&since=1751349600000&until=1751954400000\n",
      "‚û°Ô∏è [NISSAN-SUNNY.22]      ‚Ü™Ô∏è Opening sub-page: https://drive.yango.com/book?object_id=01d75403-9337-4af1-874d-68167bdeeb8b&since=1751349600000&until=1751954400000\n",
      "‚û°Ô∏è [NISSAN-SUNNY.23]      ‚Ü™Ô∏è Opening sub-page: https://drive.yango.com/book?object_id=0953d88b-a598-4920-b414-d912d0ce06ba&since=1751349600000&until=1751954400000\n",
      "‚û°Ô∏è [NISSAN-SUNNY.24] [INFO] Loaded 12 cars from Monthly: https://drive.yango.com/search/all/nissan/sunny?since=1751349600000&until=1751436000000&duration_months=9&is_monthly=true\n",
      "‚û°Ô∏è [NISSAN-SUNNY.25]    üî¢ Filtered nissan, sunny cars: 9\n",
      "‚û°Ô∏è [NISSAN-SUNNY.26]      ‚Ü™Ô∏è Opening sub-page: https://drive.yango.com/book?object_id=069a6aac-9c04-41d8-98ba-6502dd99d22c&since=1751349600000&until=1751436000000&is_monthly=true\n",
      "‚û°Ô∏è [NISSAN-SUNNY.27]      ‚Ü™Ô∏è Opening sub-page: https://drive.yango.com/book?object_id=06576274-72e6-46e8-b829-643561360680&since=1751349600000&until=1751436000000&is_monthly=true\n",
      "‚û°Ô∏è [NISSAN-SUNNY.28]            üîÅ Skipping duplicate group URL: https://drive.yango.com/book?object_id=06576274-72e6-46e8-b829-643561360680&since=1751349600000&until=1751436000000&is_monthly=true\n",
      "‚û°Ô∏è [NISSAN-SUNNY.29]            üîÅ Skipping duplicate group URL: https://drive.yango.com/book?object_id=06576274-72e6-46e8-b829-643561360680&since=1751349600000&until=1751436000000&is_monthly=true\n",
      "‚û°Ô∏è [NISSAN-SUNNY.30]      ‚Ü™Ô∏è Opening sub-page: https://drive.yango.com/book?object_id=01bed153-d941-4ed8-a069-6cdb077071b7&since=1751349600000&until=1751436000000&is_monthly=true\n",
      "‚û°Ô∏è [NISSAN-SUNNY.31]      ‚Ü™Ô∏è Opening sub-page: https://drive.yango.com/book?object_id=120f02dc-837d-40c8-a311-145d246c4927&since=1751349600000&until=1751436000000&is_monthly=true\n",
      "‚û°Ô∏è [NISSAN-SUNNY.32]      ‚Ü™Ô∏è Opening sub-page: https://drive.yango.com/book?object_id=07f2e26f-dfab-4f96-8a62-d73e2d302722&since=1751349600000&until=1751436000000&is_monthly=true\n",
      "‚û°Ô∏è [NISSAN-SUNNY.33]      ‚Ü™Ô∏è Opening sub-page: https://drive.yango.com/book?object_id=01d75403-9337-4af1-874d-68167bdeeb8b&since=1751349600000&until=1751436000000&is_monthly=true\n",
      "‚û°Ô∏è [NISSAN-SUNNY.34]      ‚Ü™Ô∏è Opening sub-page: https://drive.yango.com/book?object_id=0953d88b-a598-4920-b414-d912d0ce06ba&since=1751349600000&until=1751436000000&is_monthly=true\n",
      "‚û°Ô∏è [NISSAN-SUNNY.35]            üîÅ Skipping duplicate group URL: https://drive.yango.com/book?object_id=0953d88b-a598-4920-b414-d912d0ce06ba&since=1751349600000&until=1751436000000&is_monthly=true\n",
      "‚û°Ô∏è [NISSAN-SUNNY.36]            üîÅ Skipping duplicate group URL: https://drive.yango.com/book?object_id=0953d88b-a598-4920-b414-d912d0ce06ba&since=1751349600000&until=1751436000000&is_monthly=true\n",
      "‚û°Ô∏è [NISSAN-SUNNY.37]      ‚Ü™Ô∏è Opening sub-page: https://drive.yango.com/book?object_id=56b4acb5-9e66-44bc-8677-26bc5f6f1474&since=1751349600000&until=1751436000000&is_monthly=true\n",
      "‚û°Ô∏è [NISSAN-SUNNY.38]      ‚Ü™Ô∏è Opening sub-page: https://drive.yango.com/book?object_id=00534cef-e253-494d-b57e-f7eebe4a0c03&since=1751349600000&until=1751436000000&is_monthly=true\n",
      "‚û°Ô∏è [NISSAN-SUNNY.39] \n",
      "üõë Browser closed.\n",
      "\n",
      "üìÅ Saved: d:\\Project-Analytics\\Dubai_Projects\\DXB_Rates\\output\\yango_rentals_30062025.xlsx\n",
      "‚úÖ All parallel scraping complete.\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "import time\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from urllib.parse import unquote\n",
    "from bs4 import BeautifulSoup\n",
    "import traceback\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "# ================= LOGGING ====================\n",
    "class ThreadLogger:\n",
    "    def __init__(self):\n",
    "        self.logs = []\n",
    "        self.context = None\n",
    "        self.lock = threading.Lock()\n",
    "        self.counter = 0\n",
    "\n",
    "    def set_context(self, context):\n",
    "        self.context = context\n",
    "        self.counter = 0\n",
    "\n",
    "    def log(self, message, indent=0, emoji=\"‚û°Ô∏è\"):\n",
    "        self.counter += 1\n",
    "        prefix = f\"{'    ' * indent}{emoji} [{self.context}.{self.counter}]\"\n",
    "        with self.lock:\n",
    "            self.logs.append(f\"{prefix} {message}\")\n",
    "\n",
    "    def get_logs(self):\n",
    "        return self.logs.copy()\n",
    "\n",
    "# ========== CONFIG FILE ==========\n",
    "config_path = Path.cwd() / \"config/make_model.csv\"\n",
    "config = pd.read_csv(config_path, usecols=[\"make\", \"year\", \"yango_model\"], low_memory=False)\n",
    "\n",
    "# Filter out rows where yango_model is blank or NaN\n",
    "config = config[config[\"yango_model\"].notna() & (config[\"yango_model\"].str.strip() != \"\")]\n",
    "\n",
    "today_str = datetime.today().strftime(\"%d%m%Y\")\n",
    "filename = Path.cwd() / f\"output/yango_rentals_{today_str}.xlsx\"\n",
    "\n",
    "# ========== TIME CALCULATION ==========\n",
    "ist = pytz.timezone('Asia/Kolkata')\n",
    "from_date = datetime.now(ist).date() + timedelta(days=1)\n",
    "to_date = from_date + timedelta(days=1)\n",
    "to_date_weekly = from_date + timedelta(days=7)\n",
    "\n",
    "since = int((ist.localize(datetime(from_date.year, from_date.month, from_date.day, 11, 30, 0))\n",
    "             .astimezone(pytz.utc)).timestamp() * 1000)\n",
    "until = int((ist.localize(datetime(to_date.year, to_date.month, to_date.day, 11, 30, 0))\n",
    "             .astimezone(pytz.utc)).timestamp() * 1000)\n",
    "until_weekly = int((ist.localize(datetime(to_date_weekly.year, to_date_weekly.month, to_date_weekly.day, 11, 30, 0))\n",
    "             .astimezone(pytz.utc)).timestamp() * 1000)\n",
    "\n",
    "# ========== BROWSER FACTORY ==========\n",
    "def make_fast_firefox(headless=True):\n",
    "    options = Options()\n",
    "    options.headless = headless\n",
    "    options.page_load_strategy = \"eager\"\n",
    "    options.add_argument(\"-private\")\n",
    "\n",
    "    options.set_preference(\"permissions.default.image\", 2)\n",
    "    options.set_preference(\"dom.ipc.processCount\", 1)\n",
    "    options.set_preference(\"browser.tabs.remote.autostart\", False)\n",
    "    options.set_preference(\"network.dns.disablePrefetch\", True)\n",
    "    options.set_preference(\"network.http.use-cache\", False)\n",
    "    options.set_preference(\"toolkit.cosmeticAnimations.enabled\", False)\n",
    "    options.set_preference(\"layout.css.animation.enabled\", False)\n",
    "    options.set_preference(\"layout.css.transition.enabled\", False)\n",
    "    options.set_preference(\"general.smoothScroll\", False)\n",
    "    options.set_preference(\"ui.prefersReducedMotion\", 1)\n",
    "\n",
    "    options.binary_location = r\"C:\\Program Files\\Mozilla Firefox\\firefox.exe\"\n",
    "    service = Service(executable_path=r\"C:\\drivers\\geckodriver.exe\")\n",
    "    \n",
    "    return webdriver.Firefox(service=service, options=options)\n",
    "\n",
    "# ========== DATA CLEANING ==========\n",
    "def extract_numeric(text):\n",
    "    if pd.isnull(text):\n",
    "        return 0\n",
    "    nums = ''.join(filter(str.isdigit, text))\n",
    "    return int(nums) if nums else 0\n",
    "\n",
    "def extract_amount_per_km(text):\n",
    "    match = re.search(r\"\\b\\d+(?:\\.\\d+)?\\sper\\skm\", text, re.IGNORECASE)\n",
    "    return match.group(0) if match else None\n",
    "\n",
    "def get_unique_labels(spans):\n",
    "    return \", \".join(sorted(set(span.text.strip() for span in spans if span.text.strip())))\n",
    "\n",
    "def clean_booking_url(url):\n",
    "    return re.sub(r\"&duration_months=\\d+\", \"\", url)\n",
    "\n",
    "def extract_make_model_from_yango_url(url):\n",
    "    url = unquote(url)\n",
    "    match = re.search(r'/search/all/([^/]+)/([^/?#]+)', url)\n",
    "    if match:\n",
    "        make = match.group(1)\n",
    "        model = match.group(2)\n",
    "        return make, model\n",
    "    return None, None\n",
    "\n",
    "def scroll_to_bottom(driver, pause=2, max_attempts=3):\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    for attempt in range(max_attempts):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(pause)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "# Define custom sort orders\n",
    "contract_order = CategoricalDtype([\"daily\", \"weekly\", \"monthly\"], ordered=True)\n",
    "duration_order = CategoricalDtype([\"1 month\", \"1 months\" , \"3 months\", \"6 months\", \"9 months\", \"12 months\"], ordered=True)\n",
    "\n",
    "# ========== MAIN PAGE SCRAPER ==========\n",
    "def scrape_yango_car_data(driver, url, mode, logger):\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Set implicit wait for soft fallback\n",
    "    driver.implicitly_wait(5)\n",
    "\n",
    "    # Wait for the main container to \n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"#Card\")))\n",
    "    except:\n",
    "        print(f\"‚ö†Ô∏è Timeout waiting for listings on {url}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Scroll to bottom to load all listings\n",
    "    scroll_to_bottom(driver)\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Check if page has \"No results\" or \"No matches\"\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    heading_tags = soup.select(\"p.Heading_Title__WG8ox\")\n",
    "\n",
    "    for tag in heading_tags:\n",
    "        text = tag.get_text(strip=True).lower()\n",
    "        if \"no results found\" in text or \"no matches found\" in text:\n",
    "            logger.log(f\"üö´ ‚ùå Skipping broken main-url: {url}\")\n",
    "            return pd.DataFrame([{\n",
    "                \"sub-url\": url,\n",
    "                \"page_status\": \"not found\"\n",
    "            }])\n",
    "\n",
    "    # Extract make and model from URL\n",
    "    make, model = extract_make_model_from_yango_url(url)\n",
    "\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(3)\n",
    "\n",
    "    car_data = []\n",
    "    cars = driver.find_elements(By.CSS_SELECTOR, \"#Card\")\n",
    "\n",
    "    for car in cars:\n",
    "        try:\n",
    "            link_elem = car.find_element(By.XPATH, \"./ancestor::a[1]\")\n",
    "            car_url = link_elem.get_attribute(\"href\")\n",
    "            car_url = clean_booking_url(car_url)\n",
    "            # if mode != \"monthly\":\n",
    "                \n",
    "\n",
    "            header_spans_top = car.find_elements(By.CSS_SELECTOR, \"div.Card_LabelWrapper__zUzUR span\")\n",
    "            header_spans_bottom = car.find_elements(By.CSS_SELECTOR, \"div.Card_LabelWrapperBottom__1XVgY span\")\n",
    "            header = get_unique_labels(header_spans_top + header_spans_bottom)\n",
    "\n",
    "            year_type_text = car.find_element(By.CSS_SELECTOR, \"span.ButtonSimilarInfo_ButtonSimilarInfoPrefix___Qou3\").text.strip()\n",
    "            desc_spans = car.find_elements(By.CSS_SELECTOR, \"span.Card_CardBubble__zukT3\")\n",
    "            description = \", \".join([span.text.strip() for span in desc_spans if span.text.strip()])\n",
    "\n",
    "            rating_spans = car.find_elements(By.CSS_SELECTOR, \"div.Card_rating_wrapper__L_cLw span\")\n",
    "            ratings = \" \".join([span.text.strip() for span in rating_spans if span.text.strip()])\n",
    "\n",
    "            year_type_text_cleaned = re.search(r\"\\d{4}\", year_type_text)\n",
    "            year = int(year_type_text_cleaned.group()) if year_type_text_cleaned else None\n",
    "\n",
    "            car_data.append({\n",
    "                \"sub-url\": \"https://dubai.yango.com\" + car_url.replace('&location=ae&sublocation=db', '') if car_url.startswith(\"/\") else car_url.replace('&location=ae&sublocation=db', ''),\n",
    "                \"page_status\": \"found\",                \n",
    "                \"header\": header,\n",
    "                \"make\": make,\n",
    "                \"model\": model,\n",
    "                \"year\": year,\n",
    "                \"description\": description,\n",
    "                \"ratings\": ratings,\n",
    "                \"contract\": mode\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.log(\"[ERROR] Skipping a car:\", e)\n",
    "            continue\n",
    "\n",
    "    return pd.DataFrame(car_data)\n",
    "\n",
    "# ========== DETAIL PAGE SCRAPER ==========\n",
    "def extract_subscription_details(driver, url, contract, logger):\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Set implicit wait for soft fallback\n",
    "    driver.implicitly_wait(5)\n",
    "    \n",
    "    # Wait for the main container to \n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((\n",
    "                By.CSS_SELECTOR, \".SlotText_Title__gHEmU, .BookFormSuggestedMonths_priceDetails__DoQLL\")))\n",
    "    except:\n",
    "        print(f\"‚ö†Ô∏è Timeout waiting for listings on {url}\")\n",
    "        return [{\n",
    "            \"sub-url\": url,\n",
    "            \"page_status\": \"not found\"\n",
    "        }]\n",
    "\n",
    "    # Scroll to bottom to load all listings\n",
    "    scroll_to_bottom(driver)\n",
    "    time.sleep(5)    \n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    error_heading = soup.select_one(\"h1.Heading_Title__WG8ox\") or soup.select_one(\"h1\")\n",
    "\n",
    "    if error_heading and any(msg in error_heading.get_text(strip=True).lower() for msg in [\"page not found\", \"not available\", \"404\"]):\n",
    "        logger.log(f\"‚ùå Skipping broken sub-url: {url}\")\n",
    "        return [{\n",
    "            \"sub-url\": url,\n",
    "            \"page_status\": \"not found\"\n",
    "        }]\n",
    "\n",
    "    def get_durations():\n",
    "        duration_blocks = soup.find_all(\"div\", class_=\"Slot_Slot__qIIVX\")\n",
    "        results = []\n",
    "        for block in duration_blocks:\n",
    "            title_div = block.select_one(\".SlotText_Title__gHEmU\")\n",
    "            duration = title_div.get_text(strip=True).replace('\\xa0', ' ') if title_div else None\n",
    "            if not duration or not re.search(r\"\\d+\\s*(day|week|month)\", duration, re.IGNORECASE):\n",
    "                continue\n",
    "            if contract == \"monthly\":\n",
    "                total_sub = block.select_one(\".BookFormSuggestedMonths_withoutDiscount__zPhJI .SlotText_Subtitle__yHTPE\")\n",
    "                monthly_title = block.select_one(\".BookFormSuggestedMonths_priceDetails__DoQLL .SlotText_Title__gHEmU\")\n",
    "                savings_sub = block.select_one(\".BookFormSuggestedMonths_priceDetails__DoQLL .SlotText_Subtitle__yHTPE\")\n",
    "                offered_price = extract_numeric(monthly_title.get_text(strip=True)) if monthly_title else None\n",
    "                base_price = extract_numeric(total_sub.get_text(strip=True)) if total_sub else None\n",
    "                savings = extract_numeric(savings_sub.get_text(strip=True)) if savings_sub else None\n",
    "                results.append({\n",
    "                    \"duration\": duration,\n",
    "                    \"base_price\": base_price,\n",
    "                    \"savings\": savings,\n",
    "                    \"offered_price\": offered_price\n",
    "                })\n",
    "            else:\n",
    "                discounted_span = block.select_one(\".Price_discounted__De4vH .Price_Value__ipyGJ\")\n",
    "                original_span = block.select_one(\".Price_discounted__De4vH .Price_PriceNotDiscounted__0a3zc\")\n",
    "                duration_val = extract_numeric(duration)\n",
    "                offered = extract_numeric(discounted_span.get_text(strip=True)) if discounted_span else None\n",
    "                base = extract_numeric(original_span.get_text(strip=True)) if original_span else None\n",
    "                results.append({\n",
    "                    \"duration\": duration,\n",
    "                    \"base_price\": (base * duration_val) if base and duration_val else None,\n",
    "                    \"savings\": ((base - offered) * duration_val) if base and offered and duration_val else None,\n",
    "                    \"offered_price\": (offered * duration_val) if offered and duration_val else None\n",
    "                })\n",
    "        return results\n",
    "\n",
    "    def get_mileage():\n",
    "        result = {}\n",
    "        blocks = soup.find_all(\"div\", class_=\"Slot_Slot__qIIVX\")\n",
    "        for block in blocks:\n",
    "            title_divs = block.select(\".SlotText_Title__gHEmU\")\n",
    "            if not title_divs:\n",
    "                continue\n",
    "            if any(\"mileage\" in div.get_text(strip=True).lower() for div in title_divs):\n",
    "                subtitle_div = block.find(\"div\", class_=\"SlotText_Subtitle__yHTPE\")\n",
    "                raw_text = title_divs[-1].get_text(strip=True)\n",
    "                \n",
    "                km_match = re.search(r\"[\\d,]+\\s*km\", raw_text, re.IGNORECASE)\n",
    "                mileage_value = km_match.group().replace(\",\", \"\") if km_match else None                \n",
    "                \n",
    "                result[\"mileage\"] = mileage_value\n",
    "                result[\"mileage_note\"] = extract_amount_per_km(subtitle_div.get_text(strip=True)) if subtitle_div else None\n",
    "                break\n",
    "        return result\n",
    "\n",
    "    def get_fuel_policy():\n",
    "        result = {}\n",
    "        blocks = soup.find_all(\"div\", class_=\"Slot_Slot__qIIVX\")\n",
    "        for block in blocks:\n",
    "            title_divs = block.select(\".SlotText_Title__gHEmU\")\n",
    "            if not title_divs:\n",
    "                continue\n",
    "            if any(\"fuel policy\" in div.get_text(strip=True).lower() for div in title_divs):\n",
    "                result[\"fuel_policy\"] = title_divs[-1].get_text(strip=True)\n",
    "                break\n",
    "        return result\n",
    "\n",
    "    def get_deposit():\n",
    "        result = {\"base_deposit\": None, \"offered_deposit\": None}\n",
    "        blocks = soup.find_all(\"div\", class_=\"Slot_Slot__qIIVX\")\n",
    "        for block in blocks:\n",
    "            title_divs = block.select(\".SlotText_Title__gHEmU\")\n",
    "            if not title_divs or not any(\"deposit\" in div.get_text(strip=True).lower() for div in title_divs):\n",
    "                continue\n",
    "            offer_div = block.select_one(\"span > p > span.Text_Text__F4Wpv.Text_size_M__E57lv\")\n",
    "            strike = block.select_one(\".SlotText_strikethrough__3lJ4R .SlotText_Title__gHEmU\")\n",
    "            if strike and offer_div:\n",
    "                result[\"base_deposit\"] = extract_numeric(strike.get_text(strip=True))\n",
    "                result[\"offered_deposit\"] = extract_numeric(offer_div.get_text(strip=True))\n",
    "            else:\n",
    "                for i, div in enumerate(title_divs):\n",
    "                    if \"deposit\" in div.get_text(strip=True).lower():\n",
    "                        if i + 1 < len(title_divs):\n",
    "                            deposit_value = extract_numeric(title_divs[i + 1].get_text(strip=True))\n",
    "                            result[\"base_deposit\"] = deposit_value\n",
    "                            result[\"offered_deposit\"] = deposit_value\n",
    "                        break\n",
    "            break\n",
    "        return result\n",
    "\n",
    "    def get_other_info():\n",
    "        result = {}\n",
    "        blocks = soup.find_all(\"div\", class_=\"Island_Island__ap3Xw\")\n",
    "        for block in blocks:\n",
    "            slots = block.select(\".BookFormImportantInfo_slot__apVPj\")\n",
    "            if not slots:\n",
    "                continue\n",
    "            for slot in slots:\n",
    "                title_div = slot.select_one(\".SlotText_Title__gHEmU\")\n",
    "                if not title_div:\n",
    "                    continue\n",
    "                label = title_div.get_text(strip=True).lower()\n",
    "                if \"payment\" in label:\n",
    "                    titles = slot.select(\".SlotText_Title__gHEmU\")\n",
    "                    if len(titles) >= 2:\n",
    "                        result[\"payment_mode\"] = titles[0].get_text(strip=True)\n",
    "                        result[\"payment_options\"] = titles[1].get_text(strip=True)\n",
    "                elif \"minimum age\" in label:\n",
    "                    right_div = slot.select_one(\".SlotText_right__alLBu .SlotText_Title__gHEmU\")\n",
    "                    \n",
    "                    if right_div:\n",
    "                        raw_text = right_div.get_text(strip=True)\n",
    "                        age_value = re.sub(r\"\\s*y\\.?o\\.?\", \" years\", raw_text, flags=re.IGNORECASE)\n",
    "                    \n",
    "                        result[\"minimum_driver_age\"] = age_value\n",
    "                elif \"driving experience\" in label:\n",
    "                    right_div = slot.select_one(\".SlotText_right__alLBu .SlotText_Title__gHEmU\")\n",
    "                    result[\"minimum_driving_experience\"] = right_div.get_text(strip=True).replace(\"y.o.\", \"years\") if right_div else None\n",
    "            break\n",
    "        return result\n",
    "\n",
    "    def get_insurance():\n",
    "        result = {\"insurance_type\": None, \"insurance_detail\": None}\n",
    "        type_list = []\n",
    "        detail_list = []\n",
    "\n",
    "        titles = soup.select(\".SlotText_Title__gHEmU\")\n",
    "\n",
    "        # print(f\"üîç Total insurance title blocks found: {len(titles)}\")\n",
    "\n",
    "        for i, title_div in enumerate(titles, 1):\n",
    "            title = title_div.get_text(strip=True)\n",
    "            subtitle_div = title_div.find_next(\"div\", class_=\"SlotText_Subtitle__yHTPE\")\n",
    "            subtitle = subtitle_div.get_text(\" \", strip=True) if subtitle_div else \"\"\n",
    "\n",
    "            # Only process titles containing keywords\n",
    "            if any(k in title.lower() for k in [\"insurance\", \"cover\", \"comprehensive\"]):\n",
    "                # print(f\"‚úÖ Block {i}: Title={title}, Subtitle={subtitle}\")\n",
    "                type_list.append(title)\n",
    "                detail_list.append(subtitle or \"\")\n",
    "\n",
    "        if type_list:\n",
    "            result[\"insurance_type\"] = \", \".join(type_list)\n",
    "        if detail_list:\n",
    "            result[\"insurance_detail\"] = \", \".join(detail_list)\n",
    "\n",
    "        return result\n",
    "\n",
    "    durations = get_durations()\n",
    "    mileage_options = get_mileage()\n",
    "    fuel_policy = get_fuel_policy()\n",
    "    deposit = get_deposit()\n",
    "    insurance = get_insurance()\n",
    "    other_info = get_other_info()\n",
    "\n",
    "    enriched_data = []\n",
    "    for duration_entry in durations:\n",
    "        enriched_data.append({\n",
    "            \"sub-url\": url,\n",
    "            \"page_status\": \"found\",\n",
    "            **duration_entry,\n",
    "            **mileage_options,\n",
    "            **fuel_policy,\n",
    "            **deposit,\n",
    "            **insurance,\n",
    "            **other_info\n",
    "        })\n",
    "\n",
    "    return enriched_data\n",
    "\n",
    "duration_groups = {\n",
    "    \"1\": [1],\n",
    "    \"2-3\": [2, 3],\n",
    "    \"4-6\": [4, 5, 6],\n",
    "    \"7-9\": [7, 8, 9],\n",
    "    \"10+\": [10, 11, 12, 13]\n",
    "}\n",
    "\n",
    "def get_duration_group(duration):\n",
    "    for group, values in duration_groups.items():\n",
    "        if duration in values:\n",
    "            return group\n",
    "    return None\n",
    "\n",
    "# ========== PARALLEL SCRAPE WRAPPER ==========\n",
    "\n",
    "def scrape_main_for_make_model(make, model, logger):\n",
    "    context = f\"{make.upper()}-{model.upper()}\"\n",
    "    logger.set_context(context)\n",
    "    logger.log(f\"Started scraping for Make: {make}, Model: {model}\", 0, \"üöò\")\n",
    "\n",
    "    urls = {\n",
    "        \"daily\": f\"https://drive.yango.com/search/all/{make}/{model}?since={since}&until={until}\",\n",
    "        \"weekly\": f\"https://drive.yango.com/search/all/{make}/{model}?since={since}&until={until_weekly}\",\n",
    "        \"monthly\": f\"https://drive.yango.com/search/all/{make}/{model}?since={since}&until={until}&duration_months=9&is_monthly=true\"\n",
    "    }\n",
    "\n",
    "    driver = make_fast_firefox(headless=True)\n",
    "    local_main_dataframes, local_detail_dicts, local_seen_urls, broken_urls = [], [], set(), []\n",
    "\n",
    "    try:\n",
    "        for mode, url in urls.items():\n",
    "            df = scrape_yango_car_data(driver, url, mode=mode, logger=logger)\n",
    "            \n",
    "            if df.empty or \"page_status\" not in df.columns:\n",
    "                logger.log(f\"‚ö†Ô∏è No {mode} results found, skipping.\")\n",
    "                continue\n",
    "            \n",
    "            logger.log(f\"[INFO] Loaded {len(df)} cars from {mode.capitalize()}: {url}\")\n",
    "            \n",
    "            # Filter the un-broken urls\n",
    "            found_df = df[df[\"page_status\"] == \"found\"].drop(columns=[\"page_status\"])\n",
    "\n",
    "            # Drop the unwanted urls based on make, model and year as defined in Config\n",
    "            merge_cols = [\"make\", \"model\", \"year\"]\n",
    "            df_norm = found_df.copy()\n",
    "            \n",
    "            missing_cols = [col for col in merge_cols if col not in df_norm.columns]\n",
    "            if missing_cols:\n",
    "                logger.log(f\"‚ùå Skipping {make}-{model} ‚Äî missing columns: {missing_cols}\")\n",
    "                return [], [], logger.get_logs()\n",
    "            \n",
    "            for col in merge_cols:\n",
    "                df_norm[col] = df_norm[col].astype(str).str.upper()\n",
    "            config_norm = config.rename(columns={\"yango_model\": \"model\"}).copy()\n",
    "            for col in merge_cols:\n",
    "                config_norm[col] = config_norm[col].astype(str).str.upper()\n",
    "            filtered_df = df_norm.merge(config_norm, on=merge_cols, how=\"inner\")\n",
    "\n",
    "            # Force type\n",
    "            filtered_df[\"year\"] = pd.to_numeric(filtered_df[\"year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "            \n",
    "            # Title\n",
    "            filtered_df['title'] = filtered_df['make'].str.lower() + ' ' + filtered_df['model'].str.lower() + ' ' + filtered_df['year'].astype(str)\n",
    "            \n",
    "            if filtered_df.empty:\n",
    "                logger.log(f\"‚ö†Ô∏è No {mode} results found, inline with Config.\")\n",
    "                continue\n",
    "\n",
    "            local_main_dataframes.append(filtered_df)\n",
    "            logger.log(f\"   üî¢ Filtered {make}, {model} cars: {len(filtered_df)}\") \n",
    "\n",
    "            for _, car_row in filtered_df.iterrows():\n",
    "                sub_url = car_row[\"sub-url\"]\n",
    "                contract = car_row[\"contract\"]\n",
    "\n",
    "                if sub_url in local_seen_urls:\n",
    "                    logger.log(f\"  üîÅ Skipping already scraped: {sub_url}\")\n",
    "                    continue\n",
    "                \n",
    "                local_seen_urls.add(sub_url)\n",
    "                logger.log(f\"     ‚Ü™Ô∏è Opening sub-page: {sub_url}\")\n",
    "                \n",
    "                try:\n",
    "                    detail_df = extract_subscription_details(driver, sub_url, contract, logger=logger)\n",
    "                    local_detail_dicts.extend(detail_df)\n",
    "                    \n",
    "                    if ( not detail_df or all(\"page_status\" in d and d[\"page_status\"] == \"not found\" for d in detail_df)):\n",
    "                        logger.log(f\"üö´ Detected sub-url broken. Skipping missing duration: {sub_url}\")\n",
    "                        broken_urls.append(sub_url)\n",
    "                        continue\n",
    "                    \n",
    "                    if contract == \"monthly\":\n",
    "                        found_groups = set()\n",
    "                        for d in detail_df:\n",
    "                            group = get_duration_group(extract_numeric(d.get(\"duration\")))\n",
    "                            if group:\n",
    "                                found_groups.add(group)\n",
    "\n",
    "                        missing_groups = sorted(set(duration_groups.keys()) - found_groups)\n",
    "                        \n",
    "                        for group in missing_groups:\n",
    "                            rep_duration = duration_groups[group][0]\n",
    "                            alt_url = re.sub(r\"(duration_months=)\\d+\", lambda m: f\"{m.group(1)}{rep_duration}\", sub_url)\n",
    "\n",
    "                            if alt_url in local_seen_urls:\n",
    "                                logger.log(f\"           üîÅ Skipping duplicate group URL: {alt_url}\")\n",
    "                                continue\n",
    "                            local_seen_urls.add(alt_url)\n",
    "                            logger.log(f\"        üîÅ Missing group {group} ‚Üí duration={rep_duration} ‚Üí {alt_url}\")\n",
    "\n",
    "                            try:\n",
    "                                alt_detail = extract_subscription_details(driver, alt_url, contract, logger=logger)\n",
    "                                local_detail_dicts.extend(alt_detail)\n",
    "                            except Exception as e:\n",
    "                                logger.log(f\"‚ùå Error scraping missing group URL: {e}\")\n",
    "                                continue\n",
    "                except Exception as e:\n",
    "                    logger.log(f\"‚ùå Error scraping sub-page: {e}\")\n",
    "                    continue\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        logger.log(\"\\nüõë Browser closed.\")\n",
    "    return local_main_dataframes, local_detail_dicts, logger.get_logs()\n",
    "\n",
    "\n",
    "# ========== MAIN RUN WITH THREADING ==========\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Launching parallel scraping...\")\n",
    "\n",
    "    main_dataframes, detail_dicts, all_logs = [], [], []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        futures = []\n",
    "        unique_config = config.drop_duplicates(subset=['make', 'yango_model'])\n",
    "        for _, row in unique_config.iterrows():\n",
    "            make = row['make']\n",
    "            model = row['yango_model']\n",
    "            logger = ThreadLogger()\n",
    "            futures.append(executor.submit(scrape_main_for_make_model, make, model, logger))\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                local_main_df, local_detail_df, logs = future.result()\n",
    "                main_dataframes.extend(local_main_df)\n",
    "                detail_dicts.extend(local_detail_df)\n",
    "                all_logs.extend(logs)  # ‚¨ÖÔ∏è all logs from this thread\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Thread failed for make={make}, model={model}: {e}\")\n",
    "                traceback.print_exc()  # ‚úÖ See full error\n",
    "\n",
    "    # ========== POST PROCESSING ==========\n",
    "    print(\"\\nüìÑ Full Log Summary:\\n\" + \"=\"*40)\n",
    "    for line in all_logs:\n",
    "        print(line)\n",
    "\n",
    "    main_df = pd.concat(main_dataframes, ignore_index=True)\n",
    "    detail_df = pd.DataFrame(detail_dicts)\n",
    "\n",
    "    main_df[\"lookup\"] = main_df[\"sub-url\"].apply(clean_booking_url)\n",
    "    detail_df[\"object_id\"] = detail_df[\"sub-url\"].str.extract(r\"object_id=([a-f0-9\\-]+)\")\n",
    "    cols_to_consider = detail_df.columns.difference([\"sub-url\", \"filter\"])\n",
    "    detail_df = detail_df.drop_duplicates(subset=cols_to_consider)\n",
    "    detail_df[\"lookup\"] = detail_df[\"sub-url\"].apply(clean_booking_url)\n",
    "\n",
    "    final_df = pd.merge(main_df, detail_df.drop([\"sub-url\"], axis=1), on=\"lookup\", how=\"left\").drop(columns=[\"lookup\"])\n",
    "\n",
    "    final_df[\"duration_num\"] = final_df[\"duration\"].str.extract(r\"(\\d+)\").fillna(0).astype(int)\n",
    "\n",
    "    monthly_df = final_df[final_df[\"contract\"] == \"monthly\"].copy()\n",
    "    other_df = final_df[final_df[\"contract\"] != \"monthly\"].copy()\n",
    "\n",
    "    duration_set = set([1, 3, 6, 9, 12])\n",
    "    monthly_groups = monthly_df.groupby(\"sub-url\")\n",
    "    new_rows = []\n",
    "\n",
    "    for sub_url, group in monthly_groups:\n",
    "        existing_durations = set(group[\"duration_num\"].dropna().astype(int))\n",
    "        missing = duration_set - existing_durations\n",
    "        if not group.empty and missing:\n",
    "            base_row = group.iloc[0]\n",
    "            for m in missing:\n",
    "                row = base_row.copy()\n",
    "                row[\"duration_num\"] = m\n",
    "                row[\"duration\"] = f\"{m} months\"\n",
    "                new_rows.append(row)\n",
    "\n",
    "    if new_rows:\n",
    "        monthly_df = pd.concat([monthly_df, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "\n",
    "    monthly_df = monthly_df[monthly_df[\"duration_num\"].isin(duration_set)]\n",
    "    final_df = pd.concat([monthly_df, other_df], ignore_index=True)\n",
    "\n",
    "    duration_pattern = re.compile(r\"duration_months=\\d+\")\n",
    "    final_df[\"sub-url\"] = final_df.apply(\n",
    "        lambda row: duration_pattern.sub(f\"duration_months={int(row['duration_num'])}\", row[\"sub-url\"])\n",
    "        if \"duration_num\" in row and pd.notnull(row[\"duration_num\"]) else row[\"sub-url\"],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Drop unnecssary columns\n",
    "    final_df.drop(columns=[\"duration_num\", \"object_id\"], inplace=True)\n",
    "    \n",
    "    # Filter and process monthly contracts\n",
    "    monthly = final_df[final_df[\"contract\"] == \"monthly\"].copy()\n",
    "    monthly[\"contract\"] = monthly[\"contract\"].astype(contract_order)\n",
    "    monthly[\"duration\"] = monthly[\"duration\"].astype(duration_order)\n",
    "    monthly_sorted = monthly.sort_values(by=[\"contract\", \"sub-url\", \"duration\"]).reset_index(drop=True)\n",
    "\n",
    "    # Filter and process non-monthly contracts\n",
    "    non_monthly = final_df[final_df[\"contract\"] != \"monthly\"].copy()\n",
    "    non_monthly[\"contract\"] = non_monthly[\"contract\"].astype(contract_order)\n",
    "    non_monthly_sorted = non_monthly.sort_values(by=[\"contract\", \"sub-url\"]).reset_index(drop=True)\n",
    "\n",
    "    # Combine and sort again by contract and sub-url\n",
    "    df_sorted = pd.concat([non_monthly_sorted, monthly_sorted]).sort_values(by=[\"contract\", \"sub-url\"]).reset_index(drop=True)\n",
    "        \n",
    "    # Reorder columns\n",
    "    yango_df = df_sorted[[\n",
    "    \"sub-url\", \"title\", \"make\", \"model\", \"year\", \"ratings\", \"header\", \"contract\",\n",
    "    \"base_price\", \"savings\", \"offered_price\", \"duration\", \"mileage\", \"mileage_note\",\n",
    "    \"insurance_type\", \"insurance_detail\", \"description\", \"page_status\", \"fuel_policy\",\n",
    "    \"base_deposit\", \"offered_deposit\", \"payment_mode\", \"payment_options\",\n",
    "    \"minimum_driver_age\", \"minimum_driving_experience\"]]\n",
    "    \n",
    "    # Export as Excel\n",
    "    yango_df.to_excel(filename, index=False)\n",
    "    print(f\"\\nüìÅ Saved: {filename}\")\n",
    "    print(\"‚úÖ All parallel scraping complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
